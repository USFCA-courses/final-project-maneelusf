{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f591dd5c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- What are Large Language Models(LLMs)?<br>\n",
    "Large Language Models refer to advanced artificial intelligence systems designed to process and generate human-like text. They are trained on vast amounts of text data and learn to understand and produce natural language in a variety of contexts.\n",
    "- Is Chat GPT a large language model?<br>\n",
    "Chat GPT is a large language model which is developed by OpenAI and based on the GPT 3.5 architecture and fine-tuned on human feedback.\n",
    "- What is the architecture of Chat GPT?<br>\n",
    "Chat GPT's underlying architecture i.e. GPT 3.5 is a finetuned version of the pretrained GPT 3. GPT 3 is a transformer\n",
    "decoder architecture where its primary objective is predicting the next word. <br>\n",
    "![Alt Text](GPTarchitecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbce637",
   "metadata": {},
   "source": [
    "In the above the most important parameters are:-\n",
    "- $d_{\\texttt{model}}$: The embedding dimensions of the input text\n",
    "- $n_{\\texttt{layers}}$: The number of decoder layers present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a058add9",
   "metadata": {},
   "source": [
    "## Why is finetuning important ?\n",
    "Finetuning an Large Language Model is important as the GPT or decoder only transformers are tasked with predicting the next word. In such a case, it just tries to predict the next word in the sentence. This might not make coherent sense especially when the intention of the LLM is to be utilized as a chatbot.<br>\n",
    "Let us see check this in an example of Llama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d5d921e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkarri/anaconda3/envs/axototl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-28 18:38:04,555] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/mkarri/anaconda3/envs/axototl/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-12.1/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary /home/mkarri/anaconda3/envs/axototl/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkarri/anaconda3/envs/axototl/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/mkarri/anaconda3 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM,LlamaConfig\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import torch\n",
    "model_name_or_path = \"openlm-research/open_llama_3b\"\n",
    "config = LlamaConfig.from_pretrained(model_name_or_path)\n",
    "model = LlamaForCausalLM.from_pretrained(model_name_or_path,config = config,load_in_8bit = True,torch_dtype = torch.float16,) \n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "        model_name_or_path,trust_remote_code = True\n",
    "        )\n",
    "tokenizer.pad_token =\"[PAD]\"\n",
    "special_tokens = {'bos_token': \"<s>\",'eos_token': \"</s>\",'unk_token': \"<unk>\"}\n",
    "for k, val in special_tokens.items():\n",
    "    tokenizer.add_special_tokens({k: val})\n",
    "inputs = tokenizer(\"Tell me about Alpacas.\", return_tensors=\"pt\",truncation=False).to('cuda')\n",
    "generation_output = model.generate(**inputs,max_new_tokens=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b334a783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Tell me about Alpacas.\n",
      "Alpacas are a type of camelid, a group of animals that includes llamas, guanacos, vicunas,\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a34919",
   "metadata": {},
   "source": [
    "As we can see the model has just tried to complete the sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abb509e",
   "metadata": {},
   "source": [
    "## Model Architecture and over-view of inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9e0381",
   "metadata": {},
   "source": [
    "A traditional GPT model consists of stacked decoders. The $n_{\\texttt{layers}}$ = 96 layers listed above consists of the number of \n",
    "stacked decoders.(The decoders of the GPT 3.5 model alternate between sparse and self attention layers but more on that later)\n",
    "![Alt Text](Decoderarchitecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8a94ed",
   "metadata": {},
   "source": [
    "There are 96 of the above encoders in the GPT architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ab7a2a",
   "metadata": {},
   "source": [
    "![Alt Text](decoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caadc72",
   "metadata": {},
   "source": [
    "The model we have chosen is OpenLLamma 3B which is an Apache 2.0 version of Llamma. It replicates the Llamma architecture\n",
    "which is an open source model by Meta AI.(https://arxiv.org/abs/2302.13971). Looking at the model, we can tell the following:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4384285d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 3200, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8640, out_features=3200, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=3200, out_features=8640, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3200, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0c900d",
   "metadata": {},
   "source": [
    "In the above the most important parameters are:-\n",
    "- $d_{\\texttt{model}}$: 3200\n",
    "- $n_{\\texttt{layers}}$: 26\n",
    "<br>The main difference between the traditional decoder and the LLama paper are as below(https://paperswithcode.com/method/llama) :-\n",
    "1. RMSNorm normalizing function is used to improve the training stability, by normalizing the input of each transformer sub-layer, instead of normalizing the output.\n",
    "2. The ReLU non-linearity is replaced by the SwiGLU activation function to improve performance.\n",
    "3. Absolute positional embeddings are removed and instead rotary positional embeddings (RoPE) are added at each layer of the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5241a3f4",
   "metadata": {},
   "source": [
    "Based on the code in the lLamma paper(https://github.com/facebookresearch/llama/blob/main/llama/model.py), the main changes have been done to the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97162660",
   "metadata": {},
   "source": [
    "![Alt Text](llama.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b24672b",
   "metadata": {},
   "source": [
    "The feedforward layer has been changed as below:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbae263",
   "metadata": {},
   "source": [
    "![Alt Text](swishrelu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f979c7bc",
   "metadata": {},
   "source": [
    "Both rotary embeddings and swish Relu are out of context for this paper. For more information refer to (https://blog.eleuther.ai/rotary-embeddings/#how-is-this-different-from-the-sinusoidal-embeddings-used-in-attention-is-all-you-need) for rotary embeddings and Swish Relu(https://arxiv.org/pdf/2002.05202.pdf) The main point we are trying to make is that there have been minor changes to the original transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d6f5b6",
   "metadata": {},
   "source": [
    "## Model Inference and Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814d5800",
   "metadata": {},
   "source": [
    "Model Inference is done the following way as per the above example. There is a sequential way in which the output is determined.:-\n",
    "\n",
    "**Input**: Tell me about Alpacas. **Output**:<BOS><br>\n",
    "**Input**: Tell me about Alpacas. <BOS> **Output**: Alpacas<br>\n",
    "**Input**: Tell me about Alpacas. <BOS> Alpacas **Output**: are<br>\n",
    "**Input**: Tell me about Alpacas. <BOS> Alpacas are **Output**: a<br>\n",
    "**Input**: Tell me about Alpacas. <BOS> Alpacas are a **Output**: type<br>\n",
    "**Input**: Tell me about Alpacas. <BOS> Alpacas are a type **Output**: of<br>\n",
    "**Input**: Tell me about Alpacas. <BOS> Alpacas are a type of  **Output**: camel<br>\n",
    "**Input**: Tell me about Alpacas. <BOS> Alpacas are a type of camel **Output**: <EOS><br>\n",
    "**Final Input**: Tell me about Alpacas. <BOS> Alpacas are a type of camel **Output**: <EOS><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6e5ce4",
   "metadata": {},
   "source": [
    "Model finetuning is a supervised learning process in contrast to model training which is semi supervised. \n",
    "The cross entropy loss is the loss function. An example is shown below:-\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0e4180",
   "metadata": {},
   "source": [
    "**Input**: Tell me about Alpacas.<br>\n",
    "**Output**: Alpacas are a type of camel found in South America.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a335275",
   "metadata": {},
   "source": [
    "CE_loss = - (log(P(Alpacas)) + log(P(are/Alpacas)) + log(P(a/Alpacas are)) + log(P(type/Alpacas are a)) + log(P(camel/Alpacas are a type)))..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6cd374",
   "metadata": {},
   "source": [
    "LLMs take an enormous amount of resources to train. For e.g. from the Llama paper the 65B parameter model took 21 days on 2048 A100 GPUs. Hence the total cost of training a 65B model would take:-<br>\n",
    "2048(# of GPUs) * 24(# hours per day) * 21(# days of training) * 8.8(# Cost of GPU per hour) = $449k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0795b663",
   "metadata": {},
   "source": [
    "A100 GPUs are state of the art and are not consumer GPUs. While finetuning should likely cost a fraction of the cost, there is still a significant \n",
    "portion of compute time as well as power. Finetuning based on the Llama 7B based on the Stanford Alpaca(https://crfm.stanford.edu/2023/03/13/alpaca.html) took ~$600. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc71c1b",
   "metadata": {},
   "source": [
    "## LORA for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38362876",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac9b649",
   "metadata": {},
   "source": [
    "While we currently don't have access to a A100 GPU, a finetuning experiment will still take time. Based on the model configurations above, even ordinary finetuning can take time. Hence, in this case,\n",
    "we use LoRA(Low Rank Adapters) which ingest trainable parameters to each layer of the model. \n",
    "LORA is based on the paper(https://arxiv.org/pdf/2106.09685.pdf) and notes that during finetuning, most of the delta weights are mainly zero and hence have a \"low instrinsic dimension\". Hence, they can still learn efficiently even if projected to a smaller subpace. Hence, when learning the delta weights, we follow the following equation:-\n",
    "$h = W_0x + \\Delta Wx = W_0x + BAx$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f332d212",
   "metadata": {},
   "source": [
    "The main benefit of this is simplicity and no additional latency inference. B & A are known as the Lora weight matrices. The main parameter of the LORA adapter is the rank($r$) and the idea goes as below:-<br>\n",
    "$\\Delta Wx = M X N$<br>\n",
    "$\\Delta Wx = A_{M \\times r} X B_{r \\times N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea264018",
   "metadata": {},
   "source": [
    "## Preparing model for finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17d1b6e",
   "metadata": {},
   "source": [
    "We use the Peft package from Huggingface to assist us in the LORA finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "babe5af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from peft import prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69c4ef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_target_modules = ['gate_proj','down_proj','up_proj','q_proj','v_proj','k_proj','o_proj']\n",
    "lora_alpha = 16\n",
    "lora_r = 8\n",
    "lora_config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=lora_target_modules,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8544fd6b",
   "metadata": {},
   "source": [
    "Here, lora_r refers to the low rank taken in this case. lora_alpha refers to the alpha parameter which indicates the scaling of the weights \n",
    "when calculating gradients. The lora target modules defines the layers where Lora Adapters are being added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25fb1ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(\n",
    "            model, use_gradient_checkpointing=True\n",
    "        )\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8592f51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 12,712,960 || all params: 3,439,186,560 || trainable%: 0.36965020007521776\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c279688b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear8bitLt(\n",
       "  in_features=3200, out_features=3200, bias=False\n",
       "  (lora_dropout): ModuleDict(\n",
       "    (default): Identity()\n",
       "  )\n",
       "  (lora_A): ModuleDict(\n",
       "    (default): Linear(in_features=3200, out_features=8, bias=False)\n",
       "  )\n",
       "  (lora_B): ModuleDict(\n",
       "    (default): Linear(in_features=8, out_features=3200, bias=False)\n",
       "  )\n",
       "  (lora_embedding_A): ParameterDict()\n",
       "  (lora_embedding_B): ParameterDict()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model.model.model.layers[0].self_attn.q_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d741d72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LORA A weights\n",
      "OrderedDict([('weight', tensor([[ 0.0119, -0.0117, -0.0060,  ...,  0.0118, -0.0083,  0.0060],\n",
      "        [-0.0040, -0.0096,  0.0049,  ..., -0.0099, -0.0024, -0.0158],\n",
      "        [-0.0110, -0.0039,  0.0003,  ...,  0.0128,  0.0003, -0.0120],\n",
      "        ...,\n",
      "        [-0.0010,  0.0118,  0.0046,  ..., -0.0135,  0.0029,  0.0147],\n",
      "        [-0.0059,  0.0155, -0.0033,  ..., -0.0075,  0.0021,  0.0076],\n",
      "        [ 0.0140,  0.0060, -0.0108,  ..., -0.0110,  0.0146,  0.0043]],\n",
      "       device='cuda:0'))])\n",
      "LORA B weights\n",
      "OrderedDict([('weight', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'))])\n",
      "trainable params: 12,712,960 || all params: 3,439,186,560 || trainable%: 0.36965020007521776\n"
     ]
    }
   ],
   "source": [
    "print(\"LORA A weights\")\n",
    "print(model.base_model.model.model.layers[0].self_attn.q_proj.lora_A.default.state_dict())\n",
    "print(\"LORA B weights\")\n",
    "print(model.base_model.model.model.layers[0].self_attn.q_proj.lora_B.default.state_dict())\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b4fba9",
   "metadata": {},
   "source": [
    "The LORA B parameters are set to 0 so that the randomly initialized weights do not affect the model performance at the start of the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6e614d",
   "metadata": {},
   "source": [
    "## Preparing the Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b28af2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset, load_from_disk\n",
    "tokenizer_name = tokenizer.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf5a6f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/mkarri/.cache/huggingface/datasets/teknium___json/teknium--GPT4-LLM-Cleaned-a71aa8ae1ac3982d/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 310.80it/s]\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"teknium/GPT4-LLM-Cleaned\",streaming=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37cce5f",
   "metadata": {},
   "source": [
    "The above dataset is a refined version of the Alpaca dataset which is an instruction finetuned dataset. The below code has been adapted from the Open AI Access Collective(https://github.com/OpenAccess-AI-Collective/axolotl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d85d156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import functools\n",
    "from typing import List, Tuple, Union\n",
    "from datasets import IterableDataset\n",
    "from enum import Enum, auto\n",
    "from typing import Generator, List, Optional, Tuple, Union\n",
    "\n",
    "class InvalidDataException(Exception):\n",
    "    \"\"\"\n",
    "    Exception raised when the data is invalid\n",
    "    \"\"\"\n",
    "class PromptStyle(Enum):\n",
    "    \"\"\"\n",
    "    Enum for prompt styles\n",
    "    \"\"\"\n",
    "\n",
    "    INSTRUCT = \"instruct\"\n",
    "    CHAT = \"chat\"\n",
    "\n",
    "\n",
    "class AlpacaPrompter:\n",
    "    \"\"\"\n",
    "    Base class for alpaca prompters\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n\"\n",
    "    system_no_input_prompt = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
    "    prompt_style: Optional[PromptStyle] = None\n",
    "\n",
    "    def __init__(self, prompt_style=PromptStyle.INSTRUCT.value):\n",
    "        self.prompt_style = prompt_style if prompt_style else PromptStyle.INSTRUCT.value\n",
    "        self.match_prompt_style()\n",
    "\n",
    "    def match_prompt_style(self):\n",
    "        if self.prompt_style == PromptStyle.INSTRUCT.value:\n",
    "            self.prompt_input = (\n",
    "                self.system_prompt\n",
    "                + \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
    "            )\n",
    "            self.prompt_no_input = (\n",
    "                self.system_no_input_prompt\n",
    "                + \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "            )\n",
    "            self.response_split = \"### Response:\"\n",
    "        if self.prompt_style == PromptStyle.CHAT.value:\n",
    "            self.prompt_input = (\n",
    "                self.system_prompt + \"USER: {instruction}\\n{input}\\nASSISTANT:\"\n",
    "            )\n",
    "            self.prompt_no_input = (\n",
    "                self.system_no_input_prompt + \"USER: {instruction}\\nASSISTANT:\"\n",
    "            )\n",
    "            self.response_split = \"ASSISTANT:\"\n",
    "\n",
    "    def build_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,  # pylint: disable=redefined-builtin\n",
    "        output: Union[None, str] = None,\n",
    "    ) -> Generator[str, None, None]:\n",
    "        # returns the full prompt from instruction and optional input\n",
    "        # if a label (=response, =output) is provided, it's also appended.\n",
    "        if input:\n",
    "            res = self.prompt_input.format(instruction=instruction, input=input)\n",
    "        else:\n",
    "            res = self.prompt_no_input.format(instruction=instruction)\n",
    "        if output:\n",
    "            res = f\"{res}{output}\"\n",
    "        yield res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.split(self.response_split)[1].strip()\n",
    "\n",
    "\n",
    "\n",
    "class PromptTokenizingStrategy(abc.ABC):\n",
    "    \"\"\"\n",
    "    Abstract class for tokenizing strategies\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prompter,\n",
    "        tokenizer,\n",
    "        train_on_inputs: bool = False,\n",
    "        sequence_len: int = 2048,\n",
    "    ):\n",
    "        self.prompter = prompter\n",
    "        self.tokenizer: PreTrainedTokenizer = tokenizer\n",
    "        self.train_on_inputs = train_on_inputs\n",
    "        self.sequence_len = sequence_len\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def tokenize_prompt(self, prompt):\n",
    "        pass\n",
    "\n",
    "    @functools.lru_cache(maxsize=128)\n",
    "    def _get_user_token(self):\n",
    "        id_or_ids = self.tokenizer.convert_tokens_to_ids(\"<|USER|>\")\n",
    "        if isinstance(id_or_ids, (int,)):\n",
    "            return id_or_ids\n",
    "        return False\n",
    "\n",
    "    @functools.lru_cache(maxsize=128)\n",
    "    def _get_assistant_token(self):\n",
    "        id_or_ids = self.tokenizer.convert_tokens_to_ids(\"<|ASSISTANT|>\")\n",
    "        if isinstance(id_or_ids, (int,)):\n",
    "            return id_or_ids\n",
    "        return False\n",
    "\n",
    "    def _tokenize(self, prompt: str, add_eos_token=True, strip_bos_token=False):\n",
    "        result = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=self.sequence_len,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        if (\n",
    "            result[\"input_ids\"][-1] != self.tokenizer.eos_token_id\n",
    "            and len(result[\"input_ids\"]) < self.sequence_len\n",
    "            and add_eos_token\n",
    "        ):\n",
    "            result[\"input_ids\"].append(self.tokenizer.eos_token_id)\n",
    "            result[\"attention_mask\"].append(1)\n",
    "\n",
    "        if result[\"input_ids\"][0] == self.tokenizer.bos_token_id and strip_bos_token:\n",
    "            result[\"input_ids\"] = result[\"input_ids\"][1:]\n",
    "            result[\"attention_mask\"] = result[\"attention_mask\"][1:]\n",
    "\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "\n",
    "\n",
    "class InstructionPromptTokenizingStrategy(PromptTokenizingStrategy):\n",
    "    \"\"\"\n",
    "    Tokenizing strategy for instruction-based prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def tokenize_prompt(self, prompt):\n",
    "        (\n",
    "            instruction,\n",
    "            input,  # pylint: disable=redefined-builtin\n",
    "            response,\n",
    "        ) = self.parse_instruction_fields(prompt)\n",
    "        \n",
    "        full_prompt = self._build_full_prompt(instruction, input, response)\n",
    "        tokenized_full_prompt = self._tokenize(full_prompt)\n",
    "        if not self.train_on_inputs:\n",
    "            user_prompt = next(\n",
    "                iter(\n",
    "                    self.prompter.build_prompt(\n",
    "                        instruction,\n",
    "                        input,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            tokenized_user_prompt = self._tokenize(user_prompt, add_eos_token=False)\n",
    "            user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "            # TODO this could be sped up using numpy array slicing\n",
    "            tokenized_full_prompt[\"labels\"] = [\n",
    "                -100\n",
    "            ] * user_prompt_len + tokenized_full_prompt[\"labels\"][user_prompt_len:]\n",
    "\n",
    "        return tokenized_full_prompt\n",
    "\n",
    "    def _build_full_prompt(\n",
    "        self, instruction, input, response  # pylint: disable=redefined-builtin\n",
    "    ):\n",
    "        return next(\n",
    "            iter(\n",
    "                self.prompter.build_prompt(\n",
    "                    instruction,\n",
    "                    input,\n",
    "                    response,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "class TokenizedPromptDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Iterable dataset that returns tokenized prompts from a stream of text files.\n",
    "        Args:\n",
    "            prompt_tokenizer (PromptTokenizingStrategy): The prompt tokenizing method for proccessing the data.\n",
    "            dataset (dataset.Dataset): Dataset with text files.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(  # pylint: disable=super-init-not-called\n",
    "        self,\n",
    "        prompt_tokenizer: PromptTokenizingStrategy,\n",
    "        dataset: IterableDataset,\n",
    "    ):\n",
    "        self.prompt_tokenizer = prompt_tokenizer\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __iter__(self):\n",
    "        iterator = iter(self.dataset)\n",
    "        count = 0\n",
    "        # Loop through the entire dataset\n",
    "        for example in iterator:\n",
    "            try:\n",
    "                yield self.prompt_tokenizer.tokenize_prompt(example)\n",
    "                count += 1\n",
    "            except InvalidDataException:\n",
    "                pass\n",
    "        if count == 0:\n",
    "            raise RuntimeError(\"Expected at least one datapoint in dataset.\")\n",
    "\n",
    "class AlpacaPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n",
    "    \"\"\"\n",
    "    Tokenizing strategy for Alpaca prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n",
    "        return (\n",
    "            prompt[\"instruction\"],\n",
    "            prompt[\"input\"] if \"input\" in prompt else \"\",\n",
    "            prompt[\"output\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14cb3aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_strategy = AlpacaPromptTokenizingStrategy(AlpacaPrompter('instruct'),tokenizer,False,256)\n",
    "ds_wrapper = TokenizedPromptDataset(ds_strategy, ds['train'])\n",
    "samples = []\n",
    "for d in [ds_wrapper]:\n",
    "    samples = samples + list(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fd519f",
   "metadata": {},
   "source": [
    "So what is happening here?<br>\n",
    "**Step 1**: Create the appropriate prompt template for the dataset. Below is the prompt for the first example of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dab0fb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
      "\n",
      "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
      "\n",
      "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.\n"
     ]
    }
   ],
   "source": [
    "print(next(AlpacaPrompter('instruct').build_prompt(ds['train'][0]['instruction'],ds['train'][0]['input'],ds['train'][0]['output'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141795a3",
   "metadata": {},
   "source": [
    "**Step 2**: Create the appropriate tokenizations. Here, we can see that inputs to the model consist of both the instruction \n",
    "    as well as the answer. The labels or the output based on which the model needs to be finetuned has some -100s to it.  However, the text form of the labels denoted as  -100 noted below:-<br>\n",
    "\n",
    "<i>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "USER: Give three tips for staying healthy.</i><br>\n",
    "This is done so that we can parallely process the entire sample at one go.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ee3995",
   "metadata": {},
   "source": [
    "**Step 3**: Split into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cef6008",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_list(samples).shuffle(seed=42)\n",
    "dataset = dataset.train_test_split(test_size=0.02, shuffle=False)\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3570a4",
   "metadata": {},
   "source": [
    "## Prepare the trainer for training\n",
    "Here, we use the Huggingface Trainer which allows for a config driven management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac84aaeb",
   "metadata": {},
   "source": [
    "Now we set some learning parameters for our trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3499c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Learning Rate \n",
    "# learning_rate = 0.0002\n",
    "# # Weight decay associated with learning parameter\n",
    "# weight_decay = 0.0\n",
    "# batch_size = 16\n",
    "# micro_batch_size = 4\n",
    "# eval_steps = 50\n",
    "# save_steps = 1000\n",
    "# # Num of epochs\n",
    "# num_epochs = 3\n",
    "# # Optimizer(We use Adam 8 bit as the model has been loaded in 8 bit)\n",
    "# optimizer = 'adamw_bnb_8bit'\n",
    "# import math\n",
    "# total_num_steps = int(\n",
    "#     math.ceil(len(train_dataset) * num_epochs / batch_size)\n",
    "# )\n",
    "# warmup_steps = 10\n",
    "# logging_steps = max(min(int(0.005 * total_num_steps), 10), 1)\n",
    "# training_arguments_kwargs = {}\n",
    "# ## Train model in FP16 or float because it has been loaded in 8 bit\n",
    "# training_arguments_kwargs[\"fp16\"] = True\n",
    "# training_arguments_kwargs[\"tf32\"] = False\n",
    "# ## Gradient Checkpointing\n",
    "# gradient_checkpointing = True\n",
    "# training_arguments_kwargs[\"warmup_steps\"] = warmup_steps\n",
    "# training_arguments_kwargs[\"logging_steps\"] = logging_steps\n",
    "# training_arguments_kwargs[\"gradient_checkpointing\"] = gradient_checkpointing\n",
    "# gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "# optimizer = \"adamw_bnb_8bit\"\n",
    "# import transformers\n",
    "# training_args = transformers.TrainingArguments(\n",
    "#         per_device_train_batch_size=micro_batch_size,\n",
    "#         per_device_eval_batch_size=micro_batch_size,\n",
    "#         gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "#         eval_accumulation_steps=gradient_accumulation_steps,\n",
    "#         num_train_epochs=num_epochs,\n",
    "#         learning_rate=learning_rate,\n",
    "#         evaluation_strategy=\"steps\",\n",
    "#         save_strategy=\"steps\",\n",
    "#         eval_steps=eval_steps,\n",
    "#         save_steps=save_steps,\n",
    "#         output_dir='./lora-out',\n",
    "#         save_total_limit=3,\n",
    "#         group_by_length=False,\n",
    "#         report_to=None,\n",
    "#         run_name=None,\n",
    "#         optim=optimizer,\n",
    "#         lr_scheduler_type=\"cosine\",\n",
    "#         weight_decay=weight_decay,\n",
    "#         **training_arguments_kwargs,\n",
    "#     )\n",
    "# trainer_kwargs = {}\n",
    "# from transformers.trainer_pt_utils import get_parameter_names\n",
    "# from torch import nn\n",
    "# import bitsandbytes as bnb\n",
    "# decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
    "# decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "# optimizer_grouped_parameters = [\n",
    "#     {\n",
    "#         \"params\": [\n",
    "#             p\n",
    "#             for n, p in model.named_parameters()\n",
    "#             if (n in decay_parameters and p.requires_grad)\n",
    "#         ],\n",
    "#         \"weight_decay\": training_args.weight_decay,\n",
    "#     },\n",
    "#     {\n",
    "#         \"params\": [\n",
    "#             p\n",
    "#             for n, p in model.named_parameters()\n",
    "#             if (n not in decay_parameters and p.requires_grad)\n",
    "#         ],\n",
    "#         \"weight_decay\": 0.0,\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# optimizer = bnb.optim.Adam8bit(\n",
    "#     optimizer_grouped_parameters,\n",
    "#     betas=(training_args.adam_beta1, training_args.adam_beta2),\n",
    "#     eps=training_args.adam_epsilon,\n",
    "#     lr=training_args.learning_rate,\n",
    "# )\n",
    "# lr_scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "#                 optimizer,\n",
    "#                 training_args.warmup_steps,\n",
    "#                 total_num_steps,\n",
    "#             )\n",
    "# trainer_kwargs[\"optimizers\"] = (optimizer, lr_scheduler)\n",
    "# data_collator_kwargs = {\n",
    "#         \"padding\": True,\n",
    "#     }\n",
    "# data_collator_kwargs[\"pad_to_multiple_of\"] = 8\n",
    "# # import torch\n",
    "# # import transformers\n",
    "# # model = torch.compile(model)\n",
    "# trainer = transformers.Trainer(\n",
    "#         model=model,\n",
    "#         train_dataset=train_dataset,\n",
    "#         eval_dataset=eval_dataset,\n",
    "#         args=training_args,\n",
    "#         data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "#             tokenizer,\n",
    "#             return_tensors=\"pt\",\n",
    "#             **data_collator_kwargs,\n",
    "#         ),\n",
    "#         **trainer_kwargs,\n",
    "#     )\n",
    "# model = torch.compile(model)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1506ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate \n",
    "learning_rate = 0.0002\n",
    "# Weight decay associated with learning parameter\n",
    "weight_decay = 0.0\n",
    "batch_size = 16\n",
    "micro_batch_size = 4\n",
    "eval_steps = 50\n",
    "save_steps = 1000\n",
    "# Num of epochs\n",
    "num_epochs = 3\n",
    "# Optimizer(We use Adam 8 bit as the model has been loaded in 8 bit)\n",
    "optimizer = 'adamw_bnb_8bit'\n",
    "import math\n",
    "total_num_steps = int(\n",
    "    math.ceil(len(train_dataset) * num_epochs / batch_size)\n",
    ")\n",
    "warmup_steps = 10\n",
    "logging_steps = max(min(int(0.005 * total_num_steps), 10), 1)\n",
    "training_arguments_kwargs = {}\n",
    "## Train model in FP16 or float because it has been loaded in 8 bit\n",
    "training_arguments_kwargs[\"fp16\"] = True\n",
    "training_arguments_kwargs[\"tf32\"] = False\n",
    "## Gradient Checkpointing\n",
    "gradient_checkpointing = True\n",
    "training_arguments_kwargs[\"warmup_steps\"] = warmup_steps\n",
    "training_arguments_kwargs[\"logging_steps\"] = logging_steps\n",
    "training_arguments_kwargs[\"gradient_checkpointing\"] = gradient_checkpointing\n",
    "gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "optimizer = \"adamw_bnb_8bit\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb468df",
   "metadata": {},
   "source": [
    "We use a 8bit Adam which is quantized for 8bits. For more on quantization which is out of context. The following blog \n",
    "might help. (https://huggingface.co/blog/hf-bitsandbytes-integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a6060d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "training_args = transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=micro_batch_size,\n",
    "        per_device_eval_batch_size=micro_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        eval_accumulation_steps=gradient_accumulation_steps,\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=eval_steps,\n",
    "        save_steps=save_steps,\n",
    "        output_dir='./lora-out',\n",
    "        save_total_limit=3,\n",
    "        group_by_length=False,\n",
    "        report_to=None,\n",
    "        run_name=None,\n",
    "        optim=optimizer,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        weight_decay=weight_decay,\n",
    "        **training_arguments_kwargs,\n",
    "    )\n",
    "trainer_kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6735a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from torch import nn\n",
    "import bitsandbytes as bnb\n",
    "decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
    "decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in model.named_parameters()\n",
    "            if (n in decay_parameters and p.requires_grad)\n",
    "        ],\n",
    "        \"weight_decay\": training_args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in model.named_parameters()\n",
    "            if (n not in decay_parameters and p.requires_grad)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer = bnb.optim.Adam8bit(\n",
    "    optimizer_grouped_parameters,\n",
    "    betas=(training_args.adam_beta1, training_args.adam_beta2),\n",
    "    eps=training_args.adam_epsilon,\n",
    "    lr=training_args.learning_rate,\n",
    ")\n",
    "lr_scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                training_args.warmup_steps,\n",
    "                total_num_steps,\n",
    "            )\n",
    "trainer_kwargs[\"optimizers\"] = (optimizer, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59066962",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_kwargs = {\n",
    "        \"padding\": True,\n",
    "    }\n",
    "data_collator_kwargs[\"pad_to_multiple_of\"] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fae240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.config.use_cache = False\n",
    "import torch\n",
    "import transformers\n",
    "model = torch.compile(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47085e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        args=training_args,\n",
    "        data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "            tokenizer,\n",
    "            return_tensors=\"pt\",\n",
    "            **data_collator_kwargs,\n",
    "        ),\n",
    "        **trainer_kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f169275f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmkarri\u001b[0m (\u001b[33mwreckit\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mkarri/axotl/axotlv2/axolotl/wandb/run-20230628_184309-7pbswgnm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wreckit/huggingface/runs/7pbswgnm' target=\"_blank\">comic-waterfall-20</a></strong> to <a href='https://wandb.ai/wreckit/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wreckit/huggingface' target=\"_blank\">https://wandb.ai/wreckit/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wreckit/huggingface/runs/7pbswgnm' target=\"_blank\">https://wandb.ai/wreckit/huggingface/runs/7pbswgnm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/mkarri/anaconda3/envs/axototl/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='1431' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  35/1431 16:44 < 11:47:52, 0.03 it/s, Epoch 0.07/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/axototl/lib/python3.9/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/axototl/lib/python3.9/site-packages/transformers/trainer.py:1801\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1798\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1800\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1801\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1804\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1806\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1807\u001b[0m ):\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1809\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/axototl/lib/python3.9/site-packages/transformers/trainer.py:2644\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2642\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2643\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2644\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/anaconda3/envs/axototl/lib/python3.9/site-packages/accelerate/accelerator.py:1819\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1819\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1820\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1821\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/axototl/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/axototl/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b884a945",
   "metadata": {},
   "source": [
    "Once the model training is completed(it takes 11 hours.), the model adapter weights can be found at (https://huggingface.co/maneel/OpenLLama3B_8bitQuantized_alpaca_finetuned)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c4a49d",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d250e14e",
   "metadata": {},
   "source": [
    "Once, we have the model trained. we need to make an endpoint from it so that we can perform inference from it easily.\n",
    "The framework we have used is Ray Serve and heavily inspired from the above notebook.(https://docs.ray.io/en/latest/ray-air/examples/gptj_serving.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f693586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ray import serve\n",
    "from starlette.requests import Request\n",
    "\n",
    "\n",
    "@serve.deployment(ray_actor_options={\"num_gpus\": 1})\n",
    "class PredictDeployment:\n",
    "    def __init__(self, model_id: str, revision: str = None):\n",
    "        # from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "        from peft import PeftModel\n",
    "        from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
    "        import torch\n",
    "\n",
    "        model = LlamaForCausalLM.from_pretrained(model_id,\n",
    "                                                      torch_dtype=torch.float16, \n",
    "                                                      low_cpu_mem_usage=True,\n",
    "                                                      load_in_8bit=True, \n",
    "                                                      device_map=\"auto\")\n",
    "        #load the adapter delta weights on top of base model\n",
    "        self.model = PeftModel.from_pretrained(model, \"maneel/OpenLLama3B_8bitQuantized_alpaca_finetuned\",\\\n",
    "                                               adapter_name=\"maneel_openllama\")\n",
    "        \n",
    "        self.tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "        \n",
    "        self.generation_config = GenerationConfig(temperature=0.8,\n",
    "                                             top_p=0.75,\n",
    "                                             top_k=40,\n",
    "                                             num_beams=4,\n",
    "                                             no_repeat_ngram_size=3,\n",
    "                                             max_new_tokens=256)\n",
    "        \n",
    "    def generate(self, text: str) -> pd.DataFrame:\n",
    "        input_ids = self.tokenizer(text, return_tensors=\"pt\").input_ids.to(\n",
    "            self.model.device\n",
    "        )\n",
    "\n",
    "        gen_tokens = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=self.generation_config,\n",
    "        )\n",
    "        return pd.DataFrame(\n",
    "            self.tokenizer.batch_decode(gen_tokens), columns=[\"responses\"]\n",
    "        )\n",
    "\n",
    "    async def __call__(self, http_request: Request) -> str:\n",
    "        json_request: str = await http_request.json()\n",
    "        prompts = []\n",
    "        for prompt in json_request:\n",
    "            text = prompt[\"text\"]\n",
    "            if isinstance(text, list):\n",
    "                prompts.extend(text)\n",
    "            else:\n",
    "                prompts.append(text)\n",
    "        return self.generate(prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9ad2138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-28 19:20:21,102\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(ServeController pid=3936403)\u001b[0m INFO 2023-06-28 19:20:24,806 controller 3936403 deployment_state.py:1298 - Deploying new version of deployment default_PredictDeployment.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=3936481)\u001b[0m INFO:     Started server process [3936481]\n",
      "\u001b[2m\u001b[36m(ServeController pid=3936403)\u001b[0m INFO 2023-06-28 19:20:24,876 controller 3936403 deployment_state.py:1537 - Adding 1 replica to deployment default_PredictDeployment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeReplica:default_PredictDeployment pid=3936595)\u001b[0m [2023-06-28 19:20:28,006] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[2m\u001b[36m(ServeReplica:default_PredictDeployment pid=3936595)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ServeReplica:default_PredictDeployment pid=3936595)\u001b[0m ===================================BUG REPORT===================================\n",
      "\u001b[2m\u001b[36m(ServeReplica:default_PredictDeployment pid=3936595)\u001b[0m Welcome to bitsandbytes. For bug reports, please run\n",
      "\u001b[2m\u001b[36m(ServeReplica:default_PredictDeployment pid=3936595)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ServeReplica:default_PredictDeployment pid=3936595)\u001b[0m python -m bitsandbytes\n",
      "\u001b[2m\u001b[36m(ServeReplica:default_PredictDeployment pid=3936595)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ServeReplica:default_PredictDeployment pid=3936595)\u001b[0m  and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "\u001b[2m\u001b[36m(ServeReplica:default_PredictDeployment pid=3936595)\u001b[0m ================================================================================\n",
      "\u001b[2m\u001b[36m(ServeReplica:default_PredictDeployment pid=3936595)\u001b[0m bin /home/mkarri/anaconda3/envs/axototl/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n",
      "\u001b[2m\u001b[36m(ServeReplica:default_PredictDeployment pid=3936595)\u001b[0m CUDA SETUP: CUDA runtime path found: /usr/local/cuda-12.1/lib64/libcudart.so\n",
      "\u001b[2m\u001b[36m(ServeReplica:default_PredictDeployment pid=3936595)\u001b[0m CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "\u001b[2m\u001b[36m(ServeReplica:default_PredictDeployment pid=3936595)\u001b[0m CUDA SETUP: Detected CUDA version 121\n",
      "\u001b[2m\u001b[36m(ServeReplica:default_PredictDeployment pid=3936595)\u001b[0m CUDA SETUP: Loading binary /home/mkarri/anaconda3/envs/axototl/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeReplica:default_PredictDeployment pid=3936595)\u001b[0m /home/mkarri/anaconda3/envs/axototl/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/mkarri/anaconda3 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "\u001b[2m\u001b[36m(ServeReplica:default_PredictDeployment pid=3936595)\u001b[0m   warn(msg)\n",
      "\u001b[2m\u001b[36m(ServeReplica:default_PredictDeployment pid=3936595)\u001b[0m The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "\u001b[2m\u001b[36m(ServeController pid=3936403)\u001b[0m WARNING 2023-06-28 19:20:54,990 controller 3936403 deployment_state.py:1869 - Deployment default_PredictDeployment has 1 replicas that have taken more than 30s to initialize. This may be caused by a slow __init__ or reconfigure method.\n",
      "Downloading adapter_model.bin:   0%|          | 0.00/51.0M [00:00<?, ?B/s]\n",
      "Downloading adapter_model.bin:  21%|██        | 10.5M/51.0M [00:00<00:01, 20.6MB/s]\n",
      "Downloading adapter_model.bin:  41%|████      | 21.0M/51.0M [00:00<00:00, 33.2MB/s]\n",
      "Downloading adapter_model.bin:  62%|██████▏   | 31.5M/51.0M [00:00<00:00, 42.6MB/s]\n",
      "Downloading adapter_model.bin:  82%|████████▏ | 41.9M/51.0M [00:01<00:00, 43.2MB/s]\n",
      "Downloading adapter_model.bin: 100%|██████████| 51.0M/51.0M [00:01<00:00, 35.1MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RayServeSyncHandle(deployment='default_PredictDeployment')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"openlm-research/open_llama_3b\"  #base openllama model\n",
    "revision = \"float16\"\n",
    "deployment = PredictDeployment.bind(model_id=model_id, revision=revision)\n",
    "serve.run(deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfa0668c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'responses': '<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction:\\nExplain in simple terms how the attention mechanism of a transformer model works\\n### Response:\\nA transformer network is a type of artificial neural network that is made up of multiple layers of interconnected nodes. At the topmost layer, called the input layer, the nodes are responsible for inputting the data into the network. The nodes in this layer will take in the input, process it, and pass it on to the next layer.\\nThe next layer, also known as the output layer, takes in the processed data from the previous layer, processes it further, and passes it on. This process continues until the end of the network, where the nodes at the final layer will have processed the input data and passed it on as output.\\nOnce the data has passed through all the layers, the final output will be the processed and pre-processed data. The attention mechanism is a mechanism used in the last layer to help to guide the processing of the data. Essentially, it allows the network to pay attention to the parts of the input that are relevant to the output, and ignore the rest. This helps to improve the accuracy and performance of the model.</s>'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeReplica:default_PredictDeployment pid=3936595)\u001b[0m INFO 2023-06-28 19:29:31,585 default_PredictDeployment default_PredictDeployment#ajdfkL szbOguWTta / default replica.py:654 - __CALL__ OK 64223.7ms\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "prompt = (\n",
    "    \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "Explain in simple terms how the attention mechanism of a transformer model works\n",
    "### Response:\"\"\"\n",
    ")\n",
    "\n",
    "sample_input = {\"text\": prompt}\n",
    "\n",
    "output = requests.post(\"http://localhost:8000/\", json=[sample_input]).json()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1469c4fc",
   "metadata": {},
   "source": [
    "As we can see, the model is at an inference end-point that we can easily query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622034d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "axototl",
   "language": "python",
   "name": "axototl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
